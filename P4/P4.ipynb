{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c88a275",
   "metadata": {},
   "source": [
    "# [LELEC2870] - Machine Learning\n",
    "\n",
    "## Practical Session 4 - Deep Learning and Convolutional Neural Networks \n",
    "\n",
    "Prof. Michel Verleysen (suppl. Dr. Cyril De Bodt)<br>\n",
    "Prof. John Lee<br>\n",
    "\n",
    "**Teaching assistants :**  \n",
    "Edouard Couplet : edouard.couplet@uclouvain.be  <br>\n",
    "Dany Rimez: dany.rimez@uclouvain.be<br>\n",
    "Mathieu Simon: mathieu.simon@uclouvain.be<br>\n",
    "Maxime Zanella: maxime.zanella@uclouvain.be<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb2208",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Before starting this practical session, make sure that all necessary libraries are installed on your computer. If you attended the last practical session, you should not have to install anything - in theory ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from PIL import Image  # \"PIL\" stands for the \"pillow\" library.  \n",
    "# Pillow is a dependency of pytorch. Hence, if you installed pytorch correctly, you should not have anything else to install.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from P4_utils import (visualize_sample_images, \n",
    "                      visualize_dataset_tSNE, \n",
    "                      visualize_2Dconvolution, \n",
    "                      verify_number_of_parameters, \n",
    "                      visualize_regression_results)\n",
    "\n",
    "import warnings\n",
    "# Ignore all warning messages\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467c697",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset for this practical sessions is part of the dataset that you will use in the project. Indeed, we will use 28 by 28 pixel images of heart scans to predict the risk (i.e., the probability) of developping a heart failure within the next 10 years. We will also explore how to extract features from the images that can the be used together with other data types (i.e, the remaining part of the project dataset). As you may have guessed, if you already read the project guidelines, this dataset is entirely **synthetic**: it is about Smurfs! Here is what healthy Smurf hearts look like:\n",
    "\n",
    "<img src=\"data/smurf_heart.png\" width = \"800\">\n",
    "\n",
    "To handle image data more easily, we will define a custom dataset class which inherits from pytorch's default <code>Dataset</code> class. This class contains three methods: \n",
    "- a constructor <code>init()</code> which stores image filenames, the path of the directory where images are located and target values if specified\n",
    "- a <code>len()</code> method that simply returns the length of the dataset, which we define naturally as the number of sample images\n",
    "- a <code>getitem()</code> method that allows to retrieve elements by their index using something like <code>image = dataset[idx]</code> where <code>dataset</code> is an instance of <code>CustomDataset</code> and <code>idx</code> is the given index value\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Code</b>  <br>\n",
    "Read the code bellow and try to understand its main aspects. <br>\n",
    "Run the cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class that inherits from the PyTorch Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    # Constructor for the dataset\n",
    "    def __init__(self, images, images_directory, target=None, transform=None):\n",
    "        # Initialize the dataset with the provided data and transformation options\n",
    "        self.images = images #List of image filenames\n",
    "        self.images_directory = images_directory # Directory where images are located\n",
    "        self.target = target # Optional list of target labels\n",
    "        \n",
    "        # If no data transformation is provided, create a default transformation\n",
    "        if transform is None:\n",
    "            transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "                                            transforms.ToTensor(), # Convert the image to a PyTorch tensor\n",
    "                                            transforms.Normalize(mean=[0.5], std=[0.5]) # Normalize the image data \n",
    "                                           ])\n",
    "            \n",
    "        self.transform = transform # Store the data transformation for later use\n",
    "\n",
    "    # Define the length of the dataset (number of data samples)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    # Retrieve a specific data sample by its index\n",
    "    def __getitem__(self, idx):\n",
    "        # Construct the full path to the image file\n",
    "        image_path = os.path.join(self.images_directory, self.images[idx])\n",
    "        # Open the image using the PIL library\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Apply the data transformation if it exists\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # If target labels are provided, return both the image and the corresponding label esle return only the image\n",
    "        if self.target is not None:\n",
    "            target = self.target[idx]\n",
    "            return image, target\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5aae3",
   "metadata": {},
   "source": [
    "Great, now that we have a custom dataset class, we can easily load image samples!\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Code</b>  <br>\n",
    "Run the cell bellow to display a grid of sample images. <br>\n",
    "Can you already observe some patterns in the images that are associated with higher risks of heart failure?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Retrieve image paths\n",
    "images = pd.read_csv(\"data/Xtab1.csv\")[\"img_filename\"]\n",
    "\n",
    "# Retrieve targets/labels\n",
    "risk = pd.read_csv(\"data/Y1.csv\", header=None, names=['risk'])\n",
    "\n",
    "# Create image dataset\n",
    "dataset = CustomDataset(images, \"data/Img1\", target=risk.values)\n",
    "\n",
    "# Display of grid of sample images from the dataset\n",
    "visualize_sample_images(dataset, gridsize=(3,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da6b8e",
   "metadata": {},
   "source": [
    "Oberving patterns would perhaps be easier if we could view all images at the same time, in a condensed representation. This can be achieved using dimensionality reduction algorithms such as t-SNE (student-t stochastic neighbor embedding). We will not explain in details how t-SNE works as this will be covered later in the course. Just know that it takes as input flattened versions of images (pixel values arranged in a vector of 28x28=784 dimensions) and ouputs 2-dimensional vectors that can be represented as points in a plane. The main idea is that if two images are similar, the two corresponding points will also be close to one another in the plane. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Code</b>  <br>\n",
    "Run the cell bellow to represent all images as points in the plane (it may take a bit of time). Hover over the points with your mouse to observe corresponding images. Points are colored according to the associated risk of heart failure.<br>\n",
    "Can you observe some patterns now ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fde086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "print(\"Dataset 2D visualization --- t-SNE on pixel values\")\n",
    "visualize_dataset_tSNE(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723d553",
   "metadata": {},
   "source": [
    "### The 2D convolution\n",
    "\n",
    "Convolution is an ubiquituous operation in computer science and in all sciences in general. Its math is fascinating and if you have time to dive into it, you definitely should. In this practical session, we will only focus on a more \"graphical\" representation: convolving an image with a given kernel is essentially taking successive dot  products between the kernel and sliding portions of the image.\n",
    "\n",
    "<img src=\"data/convolution_kernel.png\" width = \"400\">\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Code</b>  <br>\n",
    "To build a stronger intuition, look at the three kernels defined bellow. Before running the cell, try to predict the effect they will have when convolved with a given image.<br>\n",
    "Now run the cell, are the results what you expected ? <br>\n",
    "Do not hesitate to play around with other custom kernels.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# choose one image in the dataset\n",
    "idx = 20\n",
    "image, _ = dataset[idx]\n",
    "\n",
    "# Define custom kernels \n",
    "# effect of kernel_1?\n",
    "kernel_1 = np.array([[1/16, 1/8, 1/16],\n",
    "                     [1/8, 1/4, 1/8],\n",
    "                     [1/16, 1/8, 1/16]])\n",
    "# effect of kernel_2?\n",
    "kernel_2 = np.array([[-1/8, 0, 1/8],\n",
    "                     [-1/4, 0, 1/4],\n",
    "                     [-1/8, 0, 1/8]])\n",
    "# effect of kernel_3?\n",
    "kernel_3 = kernel_2.T\n",
    "\n",
    "\n",
    "# Convolve image with kernel and display the result (Note that colors are used only for the purpose of visualization)\n",
    "visualize_2Dconvolution(image, kernel_1) # EXPERIMENT HERE WITH DIFFERENT KERNELS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec56bf",
   "metadata": {},
   "source": [
    "Kernels can be seen as feature detectors. Hence, in deep learning, the output of the convolution between an image and a kernel is commonly called a **feature map**. The whole point of a convolutional neural network (CNN) is to bypass hand crafted kernels/feature detectors and to learn those kernels automatically, based on data. **CNN themselves can be seen as big automatic feature extractors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b2541b",
   "metadata": {},
   "source": [
    "### A convolutional neural network (CNN)\n",
    "\n",
    "There exists a huge variety of CNN arcitectures. For the project and for this practical session, we provide you with a very simple architecture defined in the pytorch class <code>SimpleCNN</code>.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Code</b>  <br>\n",
    "Look at the code bellow and take a sheet of paper and a pen. Try to draw a schematics of the <code>SimpleCNN</code> arctitecture. Read every line of code carefully. Try to write the sizes of the tensors at each step (do not worry about the batch size for now). Refer to this <a href=\"https://poloclub.github.io/cnn-explainer/\">awesome interactive tutorial on CNNs</a> for help - or to your teaching assistants of course;-). Also note that we are solving a regression task and not a classification task. <br>\n",
    "Compare with your neighbor, if any, and finally run the cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626842a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom CNN class (in this class, we define the network architecture)  \n",
    "class SimpleCNN(nn.Module):\n",
    "    \n",
    "    # Constructor for the CNN \n",
    "    def __init__(self, n_features):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Define the layers of the CNN\n",
    "        # First convolutional layer with 1 input channel, 8 output channels, 3x3 kernel, and padding\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
    "        # First max-pooling layer with 2x2 kernel\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # Second convolutional layer with 8 input channels, 8 output channels, 3x3 kernel, and padding\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1)\n",
    "        # Second max-pooling layer with 2x2 kernel\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # Fully connected layer with input size 8*7*7 and output size n_features\n",
    "        self.fc1 = nn.Linear(8*7*7, n_features)\n",
    "        # Fully connected layer with input size n_features and output size 1\n",
    "        self.fc2 = nn.Linear(n_features, 1) \n",
    "    \n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, x):\n",
    "        # Apply the two first convolutional layers with max-pooling and ReLU activation functions\n",
    "        x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))\n",
    "        # Reshape the data for the fully connected layers\n",
    "        x = x.view(-1, 8*7*7)\n",
    "        # Pass the data through the first fully connected layer (and extract features)\n",
    "        extracted_features = self.fc1(x)\n",
    "        # Pass the extracted features through the second fully connected layer to get the final output\n",
    "        out = self.fc2(extracted_features)\n",
    "        # return output and extracted features\n",
    "        return out, extracted_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fafcf1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Question</b>  <br>\n",
    "- How many trainable parameters are there in this network? Run the cell bellow to verify your guesses. Do not try to do some binary search or simply look in the utils file, this question was already asked at an exam so you should really try to answer it by hand ;-)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51668d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(n_features=8)\n",
    "my_guess = 2870 # Provide your guess \n",
    "\n",
    "verify_number_of_parameters(my_guess, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c32002b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Other questions you should really try to answer </b>  <br>\n",
    "- What is the size of an output feature map given the input map size, the kernel size (assume a square kernel), the stride, and the padding ? <br>\n",
    "- What is the receptive field of the second max pooling kernel (self.pool2) ? <br>\n",
    "- Can you define \"Kernel\"? \"Filter\"? \"Channel\" ? Can you complete the following sentences : \"There are as many kernels in a ... as there are ...\", \"There is as many output channels as ...\"? (Note that there is no consensus on the nomenclature so do not obsess on this one)\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1b141",
   "metadata": {},
   "source": [
    "### Training a CNN\n",
    "\n",
    "Time to put everything together. Now that we have a network architecture, we can define a proper CNN model in the <code> MyCNN </code> class. This class contains three main methods:\n",
    "- a constructor <code>init()</code> which stores hyper-parameter values.\n",
    "- a <code>fit()</code> method that contains the training loop and hence that allows to train the neural network (i.e., fit its weights to the training data).\n",
    "- a <code>predict()</code> methods that uses the trained network to make predictions on the provided data. <br>\n",
    "\n",
    "These three methods are the base of most neural networks implemented in pytorch. Additionally, we define:\n",
    "- the <code>extract features()</code> method to retrieve the output of the first linear fully conncected layer (or input of the last linear layer). The dimension of this feature vector is an hyper-parameter specified by the user (default=8).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Code</b>  <br>\n",
    "Read and understand the code bellow. If you are short on time, focus on the <code>fit()</code> method. <br>\n",
    "Finally, run the cell.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5aa6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom CNN model class (in this class, we define how to train the model)\n",
    "class MyCNN(object):\n",
    "    \n",
    "    # Constructor for the custom CNN model\n",
    "    def __init__(self, n_features=8, n_epochs=25, batch_size=20, learning_rate=0.0005):\n",
    "        self.n_features = n_features\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    # Method to train the custom CNN model\n",
    "    def fit(self, images, y, data_dir):\n",
    "        \n",
    "        # Train and validation data split\n",
    "        split_ratio = 0.75\n",
    "        split_index = int(len(images) * split_ratio)\n",
    "        images_train = images[:split_index]\n",
    "        y_train = y[:split_index]\n",
    "        images_val= images[split_index:]\n",
    "        y_val = y[split_index:]\n",
    "\n",
    "        # Datasets\n",
    "        train_dataset = CustomDataset(images_train, data_dir, y_train)\n",
    "        val_dataset = CustomDataset(images_val, data_dir, y_val)\n",
    "\n",
    "        # Data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Create an instance of the SimpleCNN model\n",
    "        self.model = SimpleCNN(n_features=self.n_features)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.MSELoss(reduction='mean')\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.n_epochs):\n",
    "\n",
    "            # Set the model in training mode\n",
    "            self.model.train()\n",
    "            # Initialize running loss\n",
    "            running_loss = 0\n",
    "            # Iterate over batches of training data\n",
    "            for i, data in enumerate(train_loader):\n",
    "                inputs, labels = data\n",
    "                \n",
    "                # Forward pass: Calculate model predictions and compute the loss\n",
    "                outputs, _ = self.model(inputs)\n",
    "                loss = criterion(outputs.squeeze(),labels.float())\n",
    "                \n",
    "                # Backpropagation: Zero the gradients, calculate gradients, and update the model's parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Keep track of the running loss for this epoch\n",
    "                with torch.no_grad():\n",
    "                    running_loss += loss.item()\n",
    "            \n",
    "            # Calculate the average training loss for this epoch\n",
    "            train_loss = np.sqrt(running_loss/(i+1))\n",
    "\n",
    "            # Model evaluation on the validation set\n",
    "            # Set the model in evaluation mode\n",
    "            self.model.eval()\n",
    "            # Initialize running loss\n",
    "            running_loss = 0\n",
    "            # Iterate over batches of validation data\n",
    "            for i, data in enumerate(val_loader):\n",
    "                inputs, labels = data\n",
    "                \n",
    "                # Forward pass: Calculate model predictions (no gradient calculation)\n",
    "                with torch.no_grad():\n",
    "                    outputs, _ = self.model(inputs)\n",
    "                    loss = criterion(outputs.squeeze(), labels.float())\n",
    "                    running_loss += loss.item()\n",
    "            \n",
    "            # Calculate the average validation loss for this epoch\n",
    "            val_loss = np.sqrt(running_loss/(i+1))\n",
    "            \n",
    "            # Print the training and validation loss every 5 epochs\n",
    "            if (epoch+1)%5 == 0:\n",
    "                print(\"Epoch: {epoch:2d} | Train loss: {train:5.3f} | Val loss: {val:5.3f}\".format(epoch=epoch+1,\n",
    "                                                                                                   train=train_loss,\n",
    "                                                                                                   val=val_loss))\n",
    "    # Method to make predictions with trained SimpleCNN model            \n",
    "    def predict(self, images, data_dir):\n",
    "        \n",
    "        # Create a dataset from the input images and data directory\n",
    "        dataset = CustomDataset(images, data_dir)\n",
    "        # Create a data loader with a batch size equal to the number of input images\n",
    "        loader = DataLoader(dataset, batch_size=len(images), shuffle=False)\n",
    "        \n",
    "        # Initialize an array to store predicted values\n",
    "        y_pred = np.zeros(len(images))\n",
    "        \n",
    "        # Set the model to evaluation mode (to disable features like dropout if needed)\n",
    "        self.model.eval()\n",
    "        # Make predictions on the input images without gradient calculation\n",
    "        with torch.no_grad():\n",
    "            for inputs in loader:\n",
    "                y_pred, _ = self.model(inputs)\n",
    "        \n",
    "        # Convert the predictions to a NumPy array and reshape it\n",
    "        return y_pred.numpy().reshape(-1)\n",
    "    \n",
    "    \n",
    "    # Method to extract features from input images with trained SimpleCNN model  \n",
    "    def extract_features(self, images, data_dir):\n",
    "        \n",
    "        # Create a dataset from the input images and data directory\n",
    "        dataset = CustomDataset(images, data_dir)\n",
    "        # Create a data loader with a batch size equal to the number of input images\n",
    "        loader = DataLoader(dataset, batch_size=len(images), shuffle=False)\n",
    "        \n",
    "        # Set the model to evaluation mode (to disable features like dropout)\n",
    "        self.model.eval()\n",
    "        # Extract features from the input images without gradient calculation\n",
    "        with torch.no_grad():\n",
    "            for inputs in loader:\n",
    "                _, features = self.model(inputs)\n",
    "        \n",
    "        # Convert the extracted features to a NumPy array\n",
    "        return features.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8098b7f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Code</b>  <br>\n",
    "Run the code bellow. We perform a train/test split and fit the model to the training data. We then evaluate the model on the test data and display some basic residual/error analysis. What can you observe ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba216fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Load Xtab1 and retrieve paths to images\n",
    "images1 = pd.read_csv(\"data/Xtab1.csv\")[\"img_filename\"]\n",
    "\n",
    "# Load targets\n",
    "Y1 = pd.read_csv(\"data/Y1.csv\", header=None, names=['risk'])\n",
    "\n",
    "# Concatenate for easier train/test separation\n",
    "df = pd.concat([images1,Y1], axis=1)\n",
    "\n",
    "# Train/test split \n",
    "df_train = df.sample(frac=0.8)\n",
    "df_test = df.drop(df_train.index)\n",
    "images_train = df_train[\"img_filename\"].values\n",
    "y_train = df_train[\"risk\"].values\n",
    "images_test = df_test[\"img_filename\"].values\n",
    "y_test = df_test[\"risk\"].values\n",
    "\n",
    "# Define the number of features to extract \n",
    "n_features=8\n",
    "# Create instance of cnn model\n",
    "cnn = MyCNN(n_features=n_features, batch_size=50, n_epochs=20, learning_rate=0.0005) \n",
    "\n",
    "# Fit cnn\n",
    "cnn.fit(images_train, y_train, 'data/Img1')\n",
    "\n",
    "# Performance evaluation\n",
    "y_pred = cnn.predict(images_test, 'data/Img1')\n",
    "rmse_cnn = np.sqrt(np.mean((y_pred-y_test)**2))\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"RMSE on test set - MyCNN : {rmse:5.3f}\".format(rmse=rmse_cnn))\n",
    "print(\"-----------------------------------------------\")\n",
    "visualize_regression_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd5381",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Code</b>  <br>\n",
    "Finally, we extract 8-dimensional features from the last layer of the network and use t-SNE to project them in the 2D plane. We compare this to a t-SNE embedding of the pixel values. As before, you can hover over the points with your mouse to display corresponding images. What do you observe ?\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d788aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "print(\"Dataset 2D visualization --- t-SNE on pixel values\")\n",
    "visualize_dataset_tSNE(dataset)\n",
    "print(\"Dataset 2D visualization --- t-SNE on extracted features\")\n",
    "visualize_dataset_tSNE(dataset, extract_features=True, feature_extractor=cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2593386",
   "metadata": {},
   "source": [
    "### Bonus: self-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c4ee2",
   "metadata": {},
   "source": [
    "Since the beginning of the session, we try to predict the risk of heart failure based on heart scans. We are thus working in a **supervised** setting: we know the targets and use them to make predictions and extract features. Note that we could also have done similar things in an **unsupervised** setting, or rather **self-supervised** setting. Indeed we can define a fake target and a proxy task, and extract feature that can potentially enhance the real task (e.g., we can rotate each image by some degree and  define a proxy task that consists in predicting the degree of this rotation). You are free to explore this during the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
