{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LELEC2870] - Machine Learning\n",
    "\n",
    "## Practical Session 1 - Linear Regression (SOLUTIONS)\n",
    "\n",
    "Prof. Michel Verleysen (suppl. Dr. Cyril De Bodt)<br>\n",
    "Prof. John Lee<br>\n",
    "\n",
    "**Teaching assistants :**  \n",
    "Edouard Couplet : edouard.couplet@uclouvain.be  <br>\n",
    "Dany Rimez: dany.rimez@uclouvain.be<br>\n",
    "Mathieu Simon: mathieu.simon@uclouvain.be<br>\n",
    "Maxime Zanella: maxime.zanella@uclouvain.be<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this first exercise session, you'll implement linear regression between the target values and one of the features of the dataset.\n",
    "\n",
    "You'll then move on to the multivariate linear regression (linear regression between the target and several input features). Next, you will learn how to build a linear model on a complete dataset i.e. using all features, even thought there is no clear linear relation between the features and the target. Finally, you will implement an iterative method named Stochastic Gradient Descent (SGD) useful for optimizing objective function.\n",
    "\n",
    "We provide you a dataset for the topic covered in this session. This dataset can be found on the Moodle page of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Linear algebra notations**\n",
    "\n",
    "The output prediction of a linear regressor for an element $p$ can then be written as follows:\n",
    "            \n",
    "\\begin{equation*}\n",
    "    t_p = w_0 + w_1. x_p^1 + w_2. x_p^2 + ... + w_D. x_p^D\n",
    "\\end{equation*}\n",
    "with $D$ the number of dimensions of input vector $\\mathbf{x}$, $w_1...w_D$ the weights of each feature/dimension, and $w_0$ the independent term (=bias).\n",
    "\n",
    "A Linear regressor can be equivalently described as a simple neural network with only one layer and a linear activation.  The analytical expression of its output is \n",
    "\\begin{equation*}\n",
    "    t_p = \\sigma (\\mathbf{x}_p \\mathbf{w} + w_0).\n",
    "\\end{equation*}\n",
    "where $\\mathbf{w}$ is the weights column vector, $w_0$ is the bias, $\\sigma$ is the activation function and $t_p$ is the output of the network. In order to simplify notations, the bias is generally included in the inputs and weights vectors, i.e. $\\mathbf{x}_p$ becomes\n",
    "\\begin{equation*}\n",
    "        \\begin{pmatrix}  1 & \\mathbf{x}_p \\end{pmatrix}\n",
    "\\end{equation*}\n",
    "and $\\mathbf{w}$ becomes\n",
    "\\begin{equation*}\n",
    "    \\begin{pmatrix}  w_0 \\\\ \\mathbf{w} \\end{pmatrix}.\n",
    "\\end{equation*}\n",
    "This convention is used in the rest of this exercise session.  Since the activation is linear, the output of one-layer neural networks becomes simply\n",
    "\\begin{equation*}\n",
    "    t_p = \\mathbf{x}_p \\mathbf{w}.\n",
    "\\end{equation*}\n",
    "Notice that now, $\\mathbf{x}_p$ and $\\mathbf{w}$ are both vectors of length $(D+1)$.\n",
    "\n",
    "We can extend this to vectorial outputs (of length $T$) in the following way\n",
    "\\begin{equation*}\n",
    "    \\mathbf{t} = \\mathbf{X} \\mathbf{w},\n",
    "\\end{equation*}\n",
    "with the shape of $\\mathbf{X}$ being $T\\times (D+1)$\n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    use_seaborn = True\n",
    "    sns.set()\n",
    "except:\n",
    "    use_seaborn = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Univariate regression\n",
    "Since 10 features are available in the provided dataset, we ask you to select one (or more) of them to perform regression on in the following exercises. \n",
    "\n",
    "For each feature, **visualize** the relationship between the feature and the target by using matplotlib.scatterplot. How can you at a glance tell which features will be most useful?\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Load and visualize the dataset**\n",
    "    \n",
    "* **Plot** each feature in function of the blood sugar levels.\n",
    "* If you had to **select** 1 feature only, which one would you keep?\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scipy.io.loadmat(f\"data/diabetes.mat\")\n",
    "X = df[\"X\"]\n",
    "n_samples, n_feats = X.shape\n",
    "t = df[\"t\"]\n",
    "t_names = [\"age\", \"sex\", \"bmi\", \"blood_pressure\", \"serum_1\", \n",
    "           \"serum_2\", \"serum_3\", \"serum_4\", \"serum_5\", \"serum_6\"]\n",
    "\n",
    "# Find correlation with vector t\n",
    "# -> For ease of use, I sorted using correlation instead of doing it vizually\n",
    "#    This way you can check how good your estimate was :)\n",
    "corr_vec = np.corrcoef(X,t, rowvar=False)[-1, :n_feats]\n",
    "most_corr = np.argsort(np.abs(corr_vec))\n",
    "\n",
    "# Show in decreasing order\n",
    "fig=plt.figure(figsize=(10, 10), dpi=90)\n",
    "for i, ind in enumerate(most_corr):\n",
    "    plt.subplot(n_feats//3+1,3,i+1)\n",
    "    plt.scatter(X[:,ind], t, s=10)\n",
    "    plt.title(f\"{t_names[ind]}: {corr_vec[ind]:.4}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Univariate regression\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Pseudo-Inverse method**\n",
    "\n",
    "We can present our dataset of $P$ datapoints with $\\mathbf{t}$ the vector of the targets of length $N$ and $\\mathbf{X}$ of shape $P \\times (D+1)$ (the $+1$ is there to accommodate for the bias). We want to find the vector of weights $\\mathbf{w}$ that minimizes the Mean Squared Error (MSE) criterion. Leading to the following equation\n",
    "\\begin{alignat*}{2}\n",
    "    && E &= \\frac{1}{P} \\|\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\|^2 \\\\\n",
    "    &\\Leftrightarrow~ & \\left(\\displaystyle\\frac{\\partial E}{\\partial \\mathbf{w}} \\right) &= \\frac{2}{P}\\left( \\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right) \\mathbf{X} = \\mathbf{0}\\\\\n",
    "    &\\Leftrightarrow~ & \\mathbf{0} &= \\mathbf{X}^T\\mathbf{t} - \\mathbf{X}^T\\mathbf{X}\\mathbf{w}\\\\\n",
    "    &\\Leftrightarrow~ & \\mathbf{X}^T\\mathbf{X}\\mathbf{w} &= \\mathbf{X}^T\\mathbf{t}\\\\\n",
    "    &\\Leftrightarrow~ & \\mathbf{w} &= \\left( \\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}.\n",
    "\\end{alignat*}\n",
    "This is different from the course's definition due to the *transposed* way the matrices are defined.\n",
    "\n",
    "For this first regression you'll have to **choose** one feature. Once you have selected it (based on your results from previous exercise), you can perform linear regression. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Linear regression on one feature**\n",
    "    \n",
    "    \n",
    "* **Implement** linear regression using the pseudo-inverse method `numpy.linalg.inv`. Take a piece of paper to write down vector dimensions before blindly going into the coding part.<br>\n",
    "    \n",
    "    \n",
    "* **Visualize** the result of your regression.  In particular, compare your prediction with the actual target values.  Are the results satisfactory ?  How could you improve them?  \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MyLinearRegressor(BaseEstimator):\n",
    "    def __init__(self, add_bias=True):\n",
    "        super().__init__()\n",
    "        self.add_bias = add_bias\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.add_bias:\n",
    "            X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=-1)\n",
    "        if len(y.shape) < 2:\n",
    "            y = np.expand_dims(y, axis=-1)\n",
    "        self.coeffs = np.linalg.inv(X.T @ X)@(X.T @ y)\n",
    "        self.bias = self.coeffs[-1,0] if self.add_bias else 0\n",
    "        self.coeffs = self.coeffs[:-1,:] if self.add_bias else self.coeffs\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = X@self.coeffs + self.bias\n",
    "        y = np.squeeze(y, axis=-1)\n",
    "        return y\n",
    "    \n",
    "    def fit_predict(self, X, y):\n",
    "        return self.fit(X, y).predict(X)\n",
    "    \n",
    "    def score(self, X, y_true):\n",    
    "        # This function is not used in the rest of the code\n",
    "        y = self.predict(X)\n",
    "        return compute_rmse(y, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpredict = np.linspace(10,40)\n",
    "linreg = MyLinearRegressor()\n",
    "best_indices = most_corr[-1:]\n",
    "xtrain = X[:, best_indices]\n",
    "\n",
    "predict = linreg.fit_predict(xtrain, t)\n",
    "plt.scatter(xtrain, t)\n",
    "plt.plot(xpredict, linreg.predict(xpredict[:,None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 RMSE & Bivariate Linear Reg.\n",
    "\n",
    "We'll now step things up! Choose one more feature (so take the 2 best features), still using only visual clues. You'll now adapt the linear regressor and *train* another one taking those *2* features into account.\n",
    "\n",
    "To evaluate which model better fits the data in a regression problem a nice and simple metric to use is the RMSE. This formal criterion that can be used to show which model is able to approximate the target values more accurately (with respect to training data). \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Expression of RMSE**\n",
    "\n",
    "\\begin{equation*}\n",
    "RMSE = \\sqrt{\\frac{1}{P} |\\mathbf{t}-\\mathbf{X}\\mathbf{w}|^2 }\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where $\\mathbf{t}$ is the value of the target (ground truth) and $\\mathbf{X}\\mathbf{w}$ is the prediction (output) of a model.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Implementing the RMSE function and the bivariate LR**\n",
    "\n",
    "* **Write** the RMSE function\n",
    "* **Compute** the Linear Regression by selecting 2 features this time. Does the RMSE improve when using 2 vectors\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Root Mean Square Error\n",
    "def compute_rmse(predict, target):\n",
    "        # SOLUTION\n",
    "    if len(target.shape) == 2:\n",
    "        target = target.squeeze()\n",
    "    if len(predict.shape) == 2:\n",
    "        predict = predict.squeeze()\n",
    "    diff = target - predict\n",
    "    if len(diff.shape) == 1:\n",
    "        diff = np.expand_dims(diff, axis=-1)\n",
    "    rmse = np.sqrt(diff.T@diff / diff.shape[0])\n",
    "    return float(rmse)\n",
    "\n",
    "linreg = MyLinearRegressor()\n",
    "best_indices = most_corr[-1:]\n",
    "xtrain = X[:, best_indices]\n",
    "\n",
    "tpredict = linreg.fit_predict(xtrain, t)\n",
    "\n",
    "linreg2 = MyLinearRegressor()\n",
    "best_indices = most_corr[-2:]\n",
    "xtrain2 = X[:, best_indices]\n",
    "\n",
    "tpredict2 = linreg2.fit_predict(xtrain2, t)\n",
    "\n",
    "\n",
    "print(compute_rmse(tpredict, t))\n",
    "print(compute_rmse(tpredict2, t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Multivariate Linear Reg.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Implement multivariate LR**\n",
    "* **Sort** all the features according to their linearity with the target vector.\n",
    "* **Test** your extended framework by cumulating all the features (first 1, then 2, followed by 3, 4 etc). What do you conclude? Is there an optimal number of features.\n",
    "* **Compute** the accuracy of the models trained over the growing number of features\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 25), dpi=150)\n",
    "# plt.figure(figsize=(5, 12), dpi=75)\n",
    "\n",
    "for i in range(1,len(t_names)+1):\n",
    "    # print(i)\n",
    "    linreg = MyLinearRegressor()\n",
    "    best_indices = most_corr[-i:]\n",
    "    xx = X[:, best_indices]\n",
    "\n",
    "    predict = linreg.fit_predict(xx, t)\n",
    "\n",
    "    plt.subplot(5,2,i)\n",
    "    plt.scatter(predict, t)\n",
    "    plt.title(f'{i} best corrcoeff -> RMSE: {compute_rmse(predict, t)}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Train-test Generalization\n",
    "To test out the generalization of your linear regressor, a data separation step is necessary. You'll now split the dataset in two equal parts at random ({X1, t1} and {X2,t2}). This will allow you to build a model on the former and asses its performance on the latter.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Divide and conquer generalization errors**\n",
    "\n",
    "* **Repeat** ex. 1.4 but this time seperating your training set in 2 parts: train and test as given in the code below.\n",
    "* **Test** your model on {X2,t2}.  Is it as good as the first one? How do you explain the difference between the two? What are the key concepts behind that? \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()\n",
    "t_copy = t.copy()\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "X_perm, t_perm = unison_shuffled_copies(X_copy, t_copy)\n",
    "half_index = X_perm.shape[0]//2\n",
    "\n",
    "X1 = X_perm[:half_index, :]\n",
    "X2 = X_perm[half_index:, :]\n",
    "\n",
    "t1 = t_perm[:half_index, :]\n",
    "t2 = t_perm[half_index:, :]\n",
    "\n",
    "linreg = MyLinearRegressor()\n",
    "i = 3\n",
    "best_indices = most_corr[-i:]\n",
    "\n",
    "xx1 = X1[:, best_indices]\n",
    "xx2 = X2[:, best_indices]\n",
    "\n",
    "linreg.fit(xx1, t1)\n",
    "\n",
    "plt.figure(figsize=(10, 5), dpi=80)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "predict_X1 = linreg.predict(xx1)\n",
    "plt.scatter(predict_X1, t1)\n",
    "plt.title(f'(X1, t1) \\n {i} best corrcoeff -> RMSE: {compute_rmse(predict_X1, t1)}')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "predict_X2 = linreg.predict(xx2)\n",
    "plt.scatter(predict_X2, t2)\n",
    "plt.title(f'(X2, t2) \\n {i} best corrcoeff -> RMSE: {compute_rmse(predict_X2, t2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Stochastic Gradient Descent\n",
    "\n",
    "While the pseudo-inverse method gives the *exact* solution, it isn't always possible or feasible to inverse matrices. It remains however possible to update the parameters or weights in an iterative fashion by using Stochastic Gradient Descent (SGD). Through the computation of the gradient of the weights with respect to the objective function (= Mean Squared Error in our case) for an input $\\mathbf{x}$, it is possible to approach the optimal solution. We also refer to this objective function as the Loss ($\\mathcal{L}$). During the SGD process the point is to *minimize* this Loss.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    " \n",
    "**Back-propagation for bi-variate Linear Regression**\n",
    "\n",
    "![image.png](data/image.png)\n",
    "    \n",
    "Computational graph of a bi-variate linear regression. The red arrows represent the gradients flowing from the loss to the weights.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Solving Regression with SGD**\n",
    "* On a piece of paper **write** the missing gradient computations from the figure above\n",
    "* Once you've computed the equations, **implement** them in the code below and **check** that your solution converges to the same one as the inverse method. Do you think there is more than 1 possible optimal solution? Why?\n",
    "* Try **tuning** the meta-parameters (`n_epochs, lr, lr_annealing`). Does this make the solution better? Which meta-parameter is the most important one? And what happens when you skew the *initial* value of the parameters towards their optimal values?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySGDLinearRegressor(MyLinearRegressor):\n",
    "    def __init__(self, n_epochs=100, lr=1e-3, lr_annealing=0.99, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.lr_annealing = lr_annealing\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        assert X.shape[-1] == 2, \"Only 2 features are supported for now\"\n",
    "        if len(y.shape) < 2:\n",
    "            y = np.expand_dims(y, axis=-1)\n",
    "        \n",
    "        w0 = 1\n",
    "        w1 = 1\n",
    "        b  = -10 if self.add_bias else 0\n",
    "        lr = self.lr\n",
    "        ## Compute coeffs\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for x, ytrue in zip(X, y):\n",
    "                # Compute Gradient\n",
    "                ypred = w0*x[0] + w1*x[1] + b\n",
    "                dLdypred = 2 * (ypred-ytrue)\n",
    "                dLdb = dLdbeta = dLdypred\n",
    "                dLdw0 = dLdbeta*x[0]\n",
    "                dLdw1 = dLdbeta*x[1]\n",
    "\n",
    "                # Update weights\n",
    "                w0 = w0 - lr*dLdw0\n",
    "                w1 = w1 - lr*dLdw1\n",
    "                b  = b  - (lr*dLdb if self.add_bias else 0)\n",
    "\n",
    "                # Update lr \n",
    "                lr = lr*self.lr_annealing/(lr+self.lr_annealing)\n",
    "\n",
    "\n",
    "        self.coeffs = np.array([w0, w1])\n",
    "        self.bias = (b[0] if self.add_bias else 0)\n",
    "        return self\n",
    "best_indices = most_corr[-2:]\n",
    "xx = X[:, best_indices]\n",
    "standardized = False\n",
    "if standardized:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    xx = StandardScaler().fit_transform(xx)\n",
    "\n",
    "add_bias = True\n",
    "linreg = MyLinearRegressor(add_bias=add_bias)\n",
    "sgdlinreg = MySGDLinearRegressor(n_epochs=100, lr=1e-3, lr_annealing=0.9, add_bias=add_bias)\n",
    "\n",
    "real_score = linreg.fit(xx, t).score(xx, t)\n",
    "sgd_score = sgdlinreg.fit(xx, t).score(xx, t)\n",
    "\n",
    "print(f\"Pseudo-inverse method: {real_score:5.2f} (coeffs: {linreg.coeffs[0][0]:03.2f}, {linreg.coeffs[1][0]:03.2f}, bias={linreg.bias:03.2f})\")\n",
    "print(f\"SGD method:            {sgd_score:5.2f} (coeffs: {sgdlinreg.coeffs[0][0]:03.2f}, {sgdlinreg.coeffs[1][0]:03.2f}, bias={sgdlinreg.bias:03.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Comments\n",
    "Allthought the act of finding the coefficients of a linear regressor presents a convex problem (meaning that there is a verifiable optimal point in the loss landscape), you'll notice that SGD won't be able to easily find this optimal solution.\n",
    "However, if you remove the bias you'll notice that SGD will converge to a really close solution (you can even get way closer by better tuning of the LR and annealing rate). So this implies that it is really hard to find the optimal value for the bias, why? Simply because the LR is applied uniformly to all the parameters. Since the optimal bias is an order of magnitude farther from its initial value than the weights (>100 compared to ≃10), it can't possibly get to that value without an adaptive rescaling of the gradients (methods such as Adam, SGD with momentum have this kind of rescaling). You can try by setting -250 or -270 as the starting point for the bias and you'll notice how much closer you'll get :)\n",
    "\n",
    "In short, it's important to be able to adapt some values more than others or to choose a correct initial value.\n",
    "Do you think that using a Stadardscaler (check the sklearn docs!) to rescale the data could help your convergence? Does this transformation change the final RMSE of the linear regrossor that uses the pseudo-invers method? Try to find why (it could come in handy for the exam ;) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "6047c7caaedbef256f5727363aed5bfddf213a811ce6893954708320030a3113"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
