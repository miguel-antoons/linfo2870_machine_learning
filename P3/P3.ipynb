{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LELEC2870] - Machine Learning\n",
    "\n",
    "## Practical Session 3 - Tensors and Back-propagation\n",
    "\n",
    "Prof. Michel Verleysen (suppl. Dr. Cyril De Bodt)<br>\n",
    "Prof. John Lee<br>\n",
    "\n",
    "**Teaching assistants :**  \n",
    "Edouard Couplet : edouard.couplet@uclouvain.be  <br>\n",
    "Dany Rimez: dany.rimez@uclouvain.be<br>\n",
    "Mathieu Simon: mathieu.simon@uclouvain.be<br>\n",
    "Maxime Zanella: maxime.zanella@uclouvain.be<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from visualize import visualize2d\n",
    "import scipy\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Introduction\n",
    "\n",
    "### Back-propagation: sending information through Graphs\n",
    "In order to efficiently compute the gradient on large neural networks, all neural network frameworks construct what is referred to as a **Graph**. This Graph retains the order of operation applied to every input by storing the intermediate values and operations performed at every step. This way when the output error is computed, it can be propagated backwards through the Graph. Applying Gradient Descent with those computed gradients will nudge the weights in a better state. An example is given below\n",
    "<img src=\"images/bp1.png\">\n",
    "\n",
    "In vectorial form:\n",
    "<img src=\"images/bp1a.png\">\n",
    "\n",
    "We can also augment the number of outputs:\n",
    "<img src=\"images/bp2.png\">\n",
    "\n",
    "And write that in vectorial form:\n",
    "<img src=\"images/bp2a.png\">\n",
    "\n",
    "Every operation in a Neural Network can be approximated by a group of nodes that take either 1 or 2 inputs and produce 1 output. Thanks to the chain rule, every node in this graph can be looked at independantly. Thus to compute the gradient of the inputs, all that is needed is the gradient of the output, the value of the inputs and the inverse of the applied function. Thanks to this *divide and conquer approach*, a large and complex system, like a deep neural network, is split up in small manageable parts.\n",
    "\n",
    "### Tensors\n",
    "In the mathematical sense a Tensor is a multi-dimensional array. Some special cases are the vector (1D) and matrix (2D). In Deep Learning this term refers to a data-array whose dimensions will depend on the situation (due to the varying batch-sizes). During this practical session we'll focus on the 'simpler' Fully Connected layers, thus only 1D and 2D Tensors will be used. In a few weeks we'll expand this to 4D Tensors in order to support Convolutional layers.\n",
    "\n",
    "### Auto-grad systems\n",
    "There are 2 ways to code Back-Propagation. You could for every possible operation in a Neural Network define a module and compute the gradients by hand for every operation:\n",
    "```\n",
    "class TestModule():\n",
    "    def __init__(self, param1, param2):\n",
    "        self.param1 = param1\n",
    "        self.param2 = param2\n",
    "        self.weights = <matrix>(param1, param2)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        # f computes a function\n",
    "        return f(self.weights, x1, x2)\n",
    "    \n",
    "    def backwards(self):\n",
    "        # df computes the gradients for weights, x1 and x2 based on \n",
    "        # the partial derivative of f\n",
    "        return df(self.weights, self.x1, self.x2)\n",
    "```\n",
    "While there is nothing wrong with this approach it isn't really that handy to use since every module will need it's own forward and backward steps. Wouldn't it be great if we could do something like this\n",
    "```\n",
    "x1 = GradTensor(<data1>)\n",
    "x2 = GradTensor(<data2>)\n",
    "y = x1+x2\n",
    "z = (y*x1)+5\n",
    "```\n",
    "and have the gradients be computed automatically along the way? \n",
    "\n",
    "## 3.2 First objective\n",
    "\n",
    "The point today will be to implement a somewhat simplified **autograd system** from scratch. Such a system constructs a Graph and automatically computes the gradients based on this structure. Thus during the forward pass *dependencies* are created in order to remember the data-flow and *gradients in the form of lambda-functions* are saved. This is done because **the gradient at the start of the network can only be computed once every subsequent gradient already has been computed**!\n",
    "\n",
    "We provide you the main structure and ask you to fill in the following functions:\n",
    "* `gt_add`: Matrix addition (this one will be done by the assistant as an example)\n",
    "* `gt_mul`: Elementwise matrix multiplication\n",
    "* `gt_matmul`: Matrix multiplication \n",
    "\n",
    "In both cases you may suppose that the matrices have the *correct dimension*. Thus if you encounter a dimension problem, *your* code is at fault. To test your implementation execute the cell below, if no errors occur you can move on.\n",
    "\n",
    "Dont worry about the following 2 functions `GradTensor._add_batch_dim` and `gt_type_check`. Those functions are there to avoid errors caused by `numpy` [broadcasting](https://numpy.org/devdocs/user/theory.broadcasting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradTensor():\n",
    "    def __init__(self, data, dependencies=None, requires_grad=True, requires_update=False):\n",
    "        \"\"\" This object contains data in the form of a numpy array, but also saves its \n",
    "            gradients while computations are made\n",
    "        \n",
    "        Args:\n",
    "            data: initialized numpy.ndarray\n",
    "            dependencies:  Iterable containing GradTensors directly upstream in the computation\n",
    "                           Graph. Thus their gradient can only be computed once the gradient\n",
    "                           of @self has been computed\n",
    "            requires_grad: Does this object need a gradient\n",
    "            require_update: Is this tensor a parameter that will need updates:\n",
    "                            if True:   this GradTensor is a parameter/weight\n",
    "                            otherwise: this GradTensor is either an input or intermediate value\n",
    "        \"\"\"\n",
    "        assert hasattr(data, \"shape\"), \"Data needs to be numpy array\"\n",
    "        assert len(data.shape) <= 2, \"Tensors of 2 dimensions max are supported\"\n",
    "        assert dependencies is None or len(dependencies) <= 2, \"Backpropagation nodes should have 2 inputs at best!\"\n",
    "        \n",
    "        self.data = data\n",
    "        self.orig_shape = data.shape\n",
    "        \n",
    "        self.requires_grad = requires_grad\n",
    "        self.requires_update = requires_update\n",
    "        self.dependencies = dependencies or ()\n",
    "        \n",
    "        self.grad = 0  # Contains the value of the gradient\n",
    "        self.grad_fx = lambda: None  # Will contain a lambda function that will return the gradient\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    def add_grad(self, df):\n",
    "        \"\"\" Sets the gradient computation function\n",
    "        \n",
    "        Args:\n",
    "            df: lambda function that returns the gradient value to be added\n",
    "        \"\"\"\n",
    "        if self.requires_grad:\n",
    "            self.grad_fx = df\n",
    "    \n",
    "    def backward(self, **kwargs):\n",
    "        \"\"\" Computes the backward pass recursively and applies the computed gradient on elements of the graph\n",
    "            if they have requieres_update set to True. Can only be called on a singular value\n",
    "        \"\"\"\n",
    "        assert self.shape == (), \"Backward should only be called on single value not matrix or vector\"\n",
    "        self.grad_fx = lambda: 1.0\n",
    "        self._backward(**kwargs)        \n",
    "\n",
    "    def _backward(self, **kwargs):\n",
    "        if self.requires_grad:\n",
    "            self.grad = self.grad_fx()\n",
    "            for dep in self.dependencies:\n",
    "                dep._backward(**kwargs)\n",
    "            self.step(**kwargs)\n",
    "    \n",
    "    def step(self, lr=0.1, **kwargs):\n",
    "        if self.requires_update:               \n",
    "            self.data = self.data - lr*self.grad\n",
    "            if self.shape != self.orig_shape:\n",
    "                raise ValueError(f\"Shape shouldn't change {self.shape}, {self.orig_shape}\")\n",
    "            self.zero_grad()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"GradTensor(data:\\n{self.data}\\nshape:{self.shape})\"\n",
    "    \n",
    "    def _add_batch_dim(self, n_repeat):\n",
    "        # Adds a dimension to handle numpy broadcasting during backprop\n",
    "        expanded = GradTensor(np.tile(self.data, (n_repeat, 1)), dependencies=(self,))\n",
    "        self.add_grad(lambda: np.sum(expanded.grad, axis=0))\n",
    "        return expanded\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        return gt_transpose(self)\n",
    "    \n",
    "    def __mul__(self, other):  # Overrides * operator\n",
    "        return gt_mul(self, other)\n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    def __add__(self, other):  # Overrides + operator\n",
    "        return gt_add(self, other)\n",
    "    __radd__ = __add__\n",
    "\n",
    "    def __sub__(self, other):  # Overrides - operator\n",
    "        return gt_sub(self, other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return gt_sub(other, self)\n",
    "\n",
    "    def __matmul__(self, other):  # Overrides @ operator\n",
    "        return gt_matmul(self, other)\n",
    "    \n",
    "    def mean(self):\n",
    "        return gt_mean(self)\n",
    "    \n",
    "    def abs(self):\n",
    "        return gt_abs(self)\n",
    "    \n",
    "    def relu(self):\n",
    "        return gt_relu(self)\n",
    "\n",
    "\n",
    "def gt_type_check(dim_check=False):  # DO NOT TRY TO UNDERSTAND THIS\n",
    "    def _type_checker(f):\n",
    "        def type_checker(*args):\n",
    "            ok_args = []\n",
    "            for i, x in enumerate(args):\n",
    "                if type(x) != GradTensor:\n",
    "                    ok_args.append(GradTensor(x*np.ones_like(args[i-1].data), requires_grad=False))\n",
    "                else:\n",
    "                    ok_args.append(x)\n",
    "            _ok_args = ok_args\n",
    "            if dim_check and ok_args[0].shape!=ok_args[1].shape:\n",
    "                batch_size = ok_args[1].shape[0] if len(ok_args[0].shape)==1 else ok_args[0].shape[0]\n",
    "                _ok_args = []\n",
    "                for i, x in enumerate(ok_args):\n",
    "                    if len(x.shape) > 1:\n",
    "                        _ok_args.append(x)\n",
    "                    else:\n",
    "                        _ok_args.append(x._add_batch_dim(batch_size))\n",
    "                    \n",
    "            return f(*_ok_args)\n",
    "        return type_checker\n",
    "    return _type_checker\n",
    "        \n",
    "\n",
    "@gt_type_check()\n",
    "def gt_transpose(x):\n",
    "    z = GradTensor(x.data.T, dependencies=(x,))\n",
    "    x.add_grad(lambda: z.grad.T)\n",
    "    return z\n",
    "\n",
    "@gt_type_check()\n",
    "def gt_mean(x):\n",
    "    z = GradTensor(np.mean(x.data), dependencies=(x,))\n",
    "    x.add_grad(lambda: z.grad/np.prod(x.shape)*np.ones_like(x.data))\n",
    "    return z\n",
    "\n",
    "@gt_type_check()\n",
    "def gt_abs(x):\n",
    "    z = GradTensor(np.abs(x.data), dependencies=(x,))\n",
    "    x.add_grad(lambda: np.where(x.data>0, z.grad, -z.grad))\n",
    "    return z\n",
    "\n",
    "@gt_type_check()\n",
    "def gt_relu(x):\n",
    "    z = GradTensor(np.clip(x.data, 0, None), dependencies=(x,))\n",
    "    x.add_grad(lambda: np.where(x.data>0, z.grad, 0))\n",
    "    return z\n",
    "\n",
    "\n",
    "@gt_type_check()\n",
    "def gt_matmul(x, y):\n",
    "    ###################\n",
    "    # INSERT CODE HERE\n",
    "    # Return the matrix multiplication (x@y) for gradtensors and\n",
    "    # compute the lambda function for their gradient\n",
    "    ########################\n",
    "    return None\n",
    "\n",
    "@gt_type_check(True)\n",
    "def gt_add(x, y):\n",
    "    ###################\n",
    "    # INSERT CODE HERE\n",
    "    # Return the sum for gradtensors (x+y) and\n",
    "    # compute the lambda function for their gradient\n",
    "    ########################\n",
    "    return None\n",
    "\n",
    "@gt_type_check(True)\n",
    "def gt_sub(x, y):\n",
    "    ###################\n",
    "    # INSERT CODE HERE\n",
    "    # Return the subtraction for gradtensors (x-y) and\n",
    "    # compute the lambda function for their gradient\n",
    "    ########################\n",
    "    return None\n",
    "\n",
    "@gt_type_check(True)\n",
    "def gt_mul(x, y):\n",
    "    ###################\n",
    "    # INSERT CODE HERE\n",
    "    # Return the multiplication for gradtensors (x*y) and\n",
    "    # compute the lambda function for their gradient\n",
    "    ########################\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test functions for the GradTensor**\n",
    "\n",
    "**Execute** the code below to check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer(bias=False):\n",
    "    w = torch.rand(10,5)-0.5\n",
    "    b = torch.rand(10)*2-1\n",
    "    x = torch.rand(2,5)\n",
    "    ytrue = torch.rand(2,10)\n",
    "\n",
    "    lin = nn.Linear(5,10, bias=bias)\n",
    "    with torch.no_grad():   \n",
    "        lin.weight[:] = w\n",
    "        if bias:\n",
    "            lin.bias[:] = b\n",
    "    z = lin(x)\n",
    "    loss = torch.mean(torch.abs(z-ytrue.float()))  # L1-loss\n",
    "    loss.backward()  # Computes the gradients\n",
    "    \n",
    "    gw = GradTensor(w.numpy())\n",
    "    gb = GradTensor(b.numpy())\n",
    "    gx = GradTensor(x.numpy())\n",
    "    gytrue = GradTensor(ytrue.numpy())\n",
    "    gz = gx @ gw.T\n",
    "    if bias:\n",
    "        gz = gz + gb\n",
    "    gloss = (gz-gytrue)\n",
    "    gloss = gloss.abs().mean()\n",
    "    gloss.backward()\n",
    "\n",
    "    assert np.abs(loss.detach().numpy() - np.sum(gloss.data)) < 1e-6, \"The forward pass is incorrect\"\n",
    "    assert np.abs(lin.weight.grad.numpy() - gw.grad).mean() < 1e-6, \"The backward pass is incorrect\"\n",
    "    if bias:\n",
    "        assert np.abs(lin.bias.grad.numpy() - gb.grad).mean() < 1e-6, \"The backward pass (dL/db) is incorrect\"\n",
    "        \n",
    "    print(f\"You succesfully implemented a single layer perceptron{' (with bias)'if bias else ''}!\")\n",
    "\n",
    "def two_layers(bias=False):\n",
    "    w1 = torch.rand(10,5)\n",
    "    w2 = torch.rand(10,10)\n",
    "    b2 = torch.rand(10)*2-1\n",
    "    x = torch.rand(4,5)\n",
    "    x.requires_grad = True\n",
    "    ytrue = torch.rand(4,10)\n",
    "\n",
    "    lin1 = nn.Linear(5,10, bias=False)\n",
    "    lin2 = nn.Linear(10,10, bias=bias)\n",
    "    with torch.no_grad():   \n",
    "        lin1.weight[:] = w1\n",
    "        lin2.weight[:] = w2\n",
    "        if bias:\n",
    "            lin2.bias[:] = b2\n",
    "    z = lin2(torch.nn.functional.relu(lin1(x)))\n",
    "    loss = torch.mean(torch.abs(z-ytrue.float()))  # L1-loss\n",
    "    loss.backward()  # Computes the gradients\n",
    "    \n",
    "    gw1 = GradTensor(w1.numpy())\n",
    "    gw2 = GradTensor(w2.numpy())\n",
    "    gb2 = GradTensor(b2.numpy())\n",
    "    gx = GradTensor(x.detach().numpy())\n",
    "    gytrue = GradTensor(ytrue.numpy())\n",
    "    gz = (gx @ gw1.T).relu() @ gw2.T\n",
    "    if bias:\n",
    "        gz = gz + gb2\n",
    "    gloss = (gz-gytrue).abs().mean()\n",
    "    gloss.backward(first=True)\n",
    "    assert np.abs(loss.detach().numpy() - np.sum(gloss.data)) < 1e-6, \"The forward pass is incorrect\"\n",
    "    assert np.abs(lin1.weight.grad.numpy() - gw1.grad).mean() < 1e-6, \"The backward pass (dL/dw) is incorrect\"\n",
    "    assert np.abs(lin2.weight.grad.numpy() - gw2.grad).mean() < 1e-6, \"The backward pass (dL/dx) is incorrect\"\n",
    "    assert np.abs(x.grad.numpy() - gx.grad).mean() < 1e-6, \"The backward pass (dL/dx) is incorrect\"\n",
    "    if bias:\n",
    "        assert np.abs(lin2.bias.grad.numpy() - gb2.grad).mean() < 1e-6, \"The backward pass (dL/db) is incorrect\"\n",
    "\n",
    "    print(f\"You succesfully implemented a double layer perceptron{' (with bias)'if bias else ''}!\")\n",
    "\n",
    "one_layer()\n",
    "one_layer(bias=True)\n",
    "two_layers()\n",
    "two_layers(bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Defining Modules \n",
    "\n",
    "### Fully Connected/Linear Layer\n",
    "\n",
    "Now that you have implemented tensor operation, you can make a linear layer. Initialize the weights according to the convention set earlier. Use the `glorot_init` function to initialize the weight tensor $W$ and initialize the bias $b$ with zeros.\n",
    "\n",
    "According to the convention set during the oral Backprop explanation (see slides) $W$ should be of shape (n_out, n_in) and $b$ (n_out,). This may seem a bit inconsistent (it kind of is), but this is the way it is implemented in Deep Learning toolboxes (like pytorch) so it's better to get used to it early on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(n_in, n_out):\n",
    "    \"\"\" Returns a GradTensor of size (n_out, n_in) initialized according\n",
    "        to the Xavier Glorot based on the #input and #output neurons\n",
    "    \n",
    "    Args:\n",
    "        n_in (int): #input neurons\n",
    "        n_out (int): #output neurons\n",
    "\n",
    "    Returns:\n",
    "        Initialized GradTensor of size (n_out, n_in)\n",
    "    \"\"\"\n",
    "    size = (n_out, n_in)\n",
    "    gain = np.sqrt(2) # gain for relu activation\n",
    "    scale = gain * np.sqrt(6/(n_in+n_out))\n",
    "    return GradTensor(np.random.uniform(-scale, scale, size=size), requires_update=True)\n",
    "    \n",
    "class MyNNModule():\n",
    "    \"\"\" Super class rerouting __call__() to forward(). Thanks to that the following\n",
    "        becomes possible:\n",
    "        >>> x = data\n",
    "        >>> m = MyNNModule()\n",
    "        >>> y = m(x) # instead of: y=m.forward(x)\n",
    "    \"\"\"\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class MyLinear(MyNNModule):\n",
    "    def __init__(self, n_in, n_out, use_bias=True):\n",
    "        \"\"\" NNModule representing a Linear layer\n",
    "        \n",
    "        Args:\n",
    "            n_in: #input neurons\n",
    "            n_out: #output neurons\n",
    "            use_bias: if true adds a bias term\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #########################\n",
    "        # INSERT CODE HERE\n",
    "        # Initialize the weights with the correct dimensions\n",
    "        #########################\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #########################\n",
    "        # INSERT CODE HERE\n",
    "        # Return y where y = x@w.T + b\n",
    "        # w and b should've been initialised in __init__\n",
    "        #########################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an MLP class\n",
    "Read the function definition and implement a scalable Multi-Layer-Perceptron with a constant hidden layer size. You can extend this implementation by adding different hidden layer sizes, but it isn't needed for the further exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(MyNNModule):\n",
    "    def __init__(self, n_layers=3, hidden_size=32, input_size=1, output_size=1):\n",
    "        \"\"\" A Module that contains an MLP with a specified number of hidden layers\n",
    "            according to the given size\n",
    "            \n",
    "        Args:\n",
    "            n_layers: number of hidden layers\n",
    "            hidden_size: size of the hidden layers\n",
    "            input_size: number of input features\n",
    "            output_size: number of output features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert n_layers>0, \"At least one hidden layer is needed\"\n",
    "        #########################\n",
    "        # INSERT CODE HERE\n",
    "        # Initialize the hidden layers with the correct dimensions\n",
    "        #########################\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #########################\n",
    "        # INSERT CODE HERE\n",
    "        # compute the forward pass with all the hidden layers\n",
    "        #########################\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Training the MLP\n",
    "It is now time to train the MLP. The update function (= gradient descent update) has already been provided inside the `backward` call of a GradTensor, **read the function comments** if you didn't earlier. Your task will be to do the inference and back propagation using the following parameters\n",
    "* batch_size,\n",
    "* n_epochs;\n",
    "\n",
    "and computing the mean L1-norm as the loss.\n",
    "Per epoch shuffle the `stochastich_shuffling` array in order to process the data points randomly each iteration (same as when we shuffles the data for Clustering algortihms in Session 1). Each iteration should infer `batch_size` elements only (the last iteration may be shorter if `n_elem % batch_size > 0`)!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLPRegressor(BaseEstimator):\n",
    "    def __init__(self, n_layers=3, hidden_size=32, lr_init=1e-3, batch_size=16, n_epochs=500, milestones=None, early_stopping=10, tol=1e-4):\n",
    "        \"\"\" Wraps the MLP training loop to fit sklearn specifications for regression \n",
    "        \n",
    "        Args:\n",
    "            n_layers: #hidden layers\n",
    "            hidden_size: #neurons per hidden layer\n",
    "            lr_init: initial learning rate\n",
    "            batch_size: #samples per iteration\n",
    "            n_epochs: #epochs\n",
    "            milestones: list of ints for epochs when to divide lr by 10\n",
    "            early_stopping: #epochs after which to stop training if loss doesn't decrease\n",
    "            tol: tolerance for early stopping\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr_init = lr_init\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.milestones = milestones or [200, 400]\n",
    "        self.early_stopping = early_stopping\n",
    "        self.tol = tol\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Initializes an MLP with the set parameters and adapted to the #features in X\n",
    "        \n",
    "        Args:\n",
    "            X: a 2D array of shape (n_elem, n_features)\n",
    "            y: a 1D array of shape (n_elem,)\n",
    "        \n",
    "        Returns:\n",
    "            Itself\n",
    "        \"\"\"\n",
    "        input_size = X.shape[1]\n",
    "        batch_size = self.batch_size\n",
    "        y = y[:,np.newaxis]\n",
    "        self.mlp = MyMLP(n_layers=self.n_layers, hidden_size=self.hidden_size, input_size=input_size, output_size=1)\n",
    "        \n",
    "        n_elem = X.shape[0]\n",
    "        stochastic_shuffling = np.arange(n_elem)\n",
    "        lr = self.lr_init\n",
    "        best_loss = np.inf\n",
    "        n_nochange = 0\n",
    "        for e in range(self.n_epochs):\n",
    "            np.random.shuffle(stochastic_shuffling)\n",
    "            epoch_loss = 0 # = Avg loss over training data\n",
    "            # TODO:\n",
    "            # - divide the data in batches\n",
    "            # - per iteration process 1 batch:\n",
    "            #     - compute the forward pass\n",
    "            #     - compute the mean L1-norm\n",
    "            #     - apply the backward pass            \n",
    "            \n",
    "            \n",
    "            # Reduce learning rate by a factor of 10 on some epochs\n",
    "            if e in self.milestones:\n",
    "                lr *= 0.1\n",
    "\n",
    "            # Early stopping Mechanism\n",
    "            if (best_loss-epoch_loss) < self.tol:\n",
    "                n_nochange += 1\n",
    "            else:\n",
    "                n_nochange = 0\n",
    "                best_loss = epoch_loss\n",
    "            if n_nochange == self.early_stopping:\n",
    "                print(f\"Stopped early at epoch {e}\")\n",
    "                break\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.mlp(GradTensor(X)).data[:,0]\n",
    "    \n",
    "    def fit_predict(self, X, y):\n",
    "        return self.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 1-D Example\n",
    "\n",
    "Now that you have a functional training tool for your MLP it is time to test it on a 1D sum of guassian, function. Neural networks have 2 different type of parameters:\n",
    "* Training parameters: lr, batch_size, n_epochs, early_stopping\n",
    "* Structural parameters: n_layers, hidden_size\n",
    "\n",
    "Play with the different paramets and try to answer the following questions:\n",
    "1. Which set of parameters is more important?\n",
    "2. Do you see a link between batch_size and learning rate (lr)\n",
    "3. Can you draw conclusions about the expressivity (aka necessary size to approximate a function of set complexity) of an MLP?\n",
    "\n",
    "\n",
    "Extend your testing set to cover the x-values in $\\left[-20,20\\right]$. What do you conclude about MLP's? What can you infer about NN in general? What changes compared to the KNNRegressor?\n",
    "\n",
    "Now set #training samples to 64, 128, 256 and 512, do your optimal parameters stay the same?\n",
    "\n",
    "---\n",
    "**Note on model expressivity**\n",
    "\n",
    "We'll take a quick detour here and circle back to polynomial functions defined by\n",
    "$\\sum_{p=0}^{n}a_p\\cdot x^p $. Let's take the following example (bleu dashed line) and sample 10 points on it (bleu dots). If we add noise to them (orange crosses) to simulate a more realistic scenario and interpolate them with a $2^{nd}$ degree polynomial we obtain a fairly accurate result (orange full line).\n",
    "\n",
    "<img src=\"images/int2.png\">\n",
    "\n",
    "However if we increase the degree of interpolation further we see the approximation becoming worse and worse.\n",
    "<img src=\"images/int4.png\"><img src=\"images/int6.png\"><img src=\"images/int9.png\">\n",
    "\n",
    "Theoretically these models should be able to approximate 2nd degree polynomials too but due to optimization issues they don't. The last image is a 9th degree polynomial, thus it is able to exactly interpolate every point, leading to an RMSE of zero. To avoid this problem it is important to not only otpimize 1 metric, for example we could add an l2 regularization term on the $a_p$ coefficients to limit this explosion. (For linear regression this is called Lasso Regression, for Neural Networks we refer to Weight Decay).\n",
    "\n",
    "In conclusion: allthough models with higher expressivity approximate more functions, they are prone to overfiting on *simpler* problems. Thus regularization is needed to keep things balanced\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_with_noise(x, mu=None, sigma=None):\n",
    "    mu = mu or [0]\n",
    "    sigma = sigma or [1]\n",
    "    assert len(mu)==len(sigma), \"Mu and sigma should have the same size\"\n",
    "    y = np.zeros_like(x)\n",
    "    for m,s in zip(mu, sigma):\n",
    "        y = y + 1/(s*np.sqrt(2*np.pi))*np.exp(-(x-m)**2/(2*s**2))\n",
    "    return y\n",
    "\n",
    "n_elem = 10000\n",
    "x = np.linspace(-10,10,n_elem)\n",
    "y = gauss_with_noise(x,\n",
    "#     mu=[-2, 0, 1, 4, 4.5, 6, 9],\n",
    "    mu=[-2, 0, 1, 3.5, 6],\n",
    "#     sigma=[2, 5, 0.2, 3, 1, 0.7, 0.2])\n",
    "    sigma=[1, 5, 2, 5, 2])\n",
    "plt.figure()\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot_mlp(mlp, x_train, y_train, x_tot, y_tot):\n",
    "    s = time.time()\n",
    "    mlp.fit(x_train, y_train)\n",
    "    e = time.time()\n",
    "    print(f\"Took {e-s:6.3f} seconds for the training\")\n",
    "    y_predict = mlp.predict(x_train)\n",
    "    loss = np.abs(y_predict-y_train).mean()\n",
    "    \n",
    "    y_predict_tot = mlp.predict(x_tot)\n",
    "    loss_tot = np.abs(y_predict_tot-y_tot).mean()\n",
    "\n",
    "    train = (x_train, y_train, y_predict, loss)\n",
    "    test = (x_tot, y_tot, y_predict_tot, loss_tot)\n",
    "    if x_train.shape[1]==1:\n",
    "        visualize2d(train, test)\n",
    "    else:\n",
    "        visualize3d(train, test)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Select Training data\n",
    "np.random.seed(0)\n",
    "n_select = 32\n",
    "selected = np.arange(0,n_elem)\n",
    "np.random.shuffle(selected)\n",
    "selected = selected[:n_select]\n",
    "x_train = x[selected][:,np.newaxis]\n",
    "y_train = y[selected]\n",
    "\n",
    "# Sklearn MLP\n",
    "# TODO: you can try and tune the meta-parameters here too to reflect the ones you use for the MyMLPRegressor\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(50,)*4, batch_size=8, learning_rate_init=5e-2, # TODO: adapt the meta-parameters on this line\n",
    "    solver='sgd', activation='relu', max_iter=500, early_stopping=False, n_iter_no_change=10 # DO NOT change these\n",
    ")\n",
    "train_plot_mlp(mlp, x_train, y_train, x[:,np.newaxis], y)\n",
    "\n",
    "# Our MLP\n",
    "# TODO: play with the meta-parameters here :)\n",
    "mlp = MyMLPRegressor(\n",
    "    n_layers=4, hidden_size=50, lr_init=1e-1, batch_size=8, n_epochs=200, early_stopping=100,\n",
    "    milestones=[200,300]\n",
    ")\n",
    "train_plot_mlp(mlp, x_train, y_train, x[:,np.newaxis], y)\n",
    "               \n",
    "# KNN\n",
    "rbfn = KNeighborsRegressor(3, weights=\"distance\")\n",
    "train_plot_mlp(rbfn, x_train, y_train, x2[:,np.newaxis], y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Conlusions about simple Fully-Connected Neural Networks\n",
    "\n",
    "Benefits:\n",
    "* Can in *theory* approximate any function\n",
    "* Handy for feature extraction (see next session)\n",
    "\n",
    "Drawbacks:\n",
    "* Really hard to train\n",
    "* No guarantees on result\n",
    "* Regularisation is hard, meta-parameter search is really important\n",
    "* Hard to measure the *expressivity* of a Neural Network\n",
    "* Hard to understand inner workings\n",
    "\n",
    "## 3.6 Further reading\n",
    "Training large neural networks is hard and if you ever want to do it more seriously you'll need to look into more specialized training mechanism:\n",
    "* SGD w/ momentum: Trains faster\n",
    "* Adam(W): Trains even faster\n",
    "* Weight Decay: Avoids explosion of weight's absolute magnitude\n",
    "\n",
    "As well as research which network structures work for your specific problem:\n",
    "* LSTM and Transformers: Natural language processing tasks\n",
    "* ConvNets (ResNets, VGG, DenseNet etc.): Image processing tasks\n",
    "* Fully connected networks: Any use case not covered by more specific networks (i.e. a vector as input and a vector as output)\n",
    "\n",
    "Finally you may want to gain some insight into the inner workings of your neural network. There are a lot of great tools with more and more being made by the minute. Never skip this step if you ever want to deploy a NN on a real-life project, you may be unaware of some of its failure cases: NN are bad at extrapolating outside of their domain!\n",
    "\n",
    "\n",
    "Furthermore this field is exploding in popularity in the last 5-10 years, thus there are a lot of papers out there claiming a lot of stuff. Always keep a healthy dose of scepticism with regards to published results especially whenever the source code may not be available: reproducing a DL-paper is more work than it may originally seem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
